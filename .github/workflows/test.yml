name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        test-group: [orchestration, policy, inventory, basic]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-cov pytest-benchmark bandit safety
          
      - name: Run static security analysis
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
          safety check --json --output safety-report.json || true
          
      - name: Run unit tests
        run: |
          pytest tests/ \
            -m "not integration and not slow and not edge_case and not performance" \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=unit-test-results.xml \
            -v
            
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unit-tests
          name: codecov-umbrella
          fail_ci_if_error: false
          
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.test-group }}
          path: |
            unit-test-results.xml
            coverage.xml
            bandit-report.json
            safety-report.json

  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: tailops_test
          POSTGRES_USER: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          ports:
            - 5432:5432
            
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-integration-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-cov psycopg2-binary
          
      - name: Wait for PostgreSQL
        run: |
          until pg_isready -h localhost -p 5432 -U test; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          
      - name: Run integration tests
        env:
          TEST_DATABASE_URL: postgresql://test:test@localhost:5432/tailops_test
          TAILOPS_TEST_MODE: true
          TAILOPS_TEST_DATABASE: postgresql://test:test@localhost:5432/tailops_test
        run: |
          pytest tests/ \
            -m integration \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=integration-test-results.xml \
            -v
            
      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            integration-test-results.xml
            coverage.xml

  security-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install security testing tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety semgrep
          pip install -r requirements.txt
          
      - name: Run Bandit security scan
        run: |
          bandit -r src/ -f json -o bandit-security-report.json || true
          bandit -r src/ -f txt
          
      - name: Run Safety vulnerability scan
        run: |
          safety check --json --output safety-vulnerability-report.json || true
          safety check
          
      - name: Run Semgrep security scan
        run: |
          semgrep --config=auto --json --output=semgrep-report.json src/ || true
          
      - name: Run security tests
        run: |
          pytest tests/test_security.py \
            -m security \
            --junitxml=security-test-results.xml \
            -v
            
      - name: Upload security test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-test-results
          path: |
            security-test-results.xml
            bandit-security-report.json
            safety-vulnerability-report.json
            semgrep-report.json

  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install performance testing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark memory-profiler psutil
          pip install -r requirements-dev.txt
          
      - name: Run performance tests
        run: |
          pytest tests/test_performance.py \
            -m performance \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --junitxml=performance-test-results.xml \
            -v
            
      - name: Upload performance test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            performance-test-results.xml
            benchmark-results.json

  edge-case-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      matrix:
        test-group: [network, auth, data, recovery]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
      - name: Run edge case tests
        run: |
          pytest tests/test_edge_cases.py \
            -m edge_case \
            --junitxml=edge-case-test-results.xml \
            -v
            
      - name: Upload edge case test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: edge-case-test-results-${{ matrix.test-group }}
          path: edge-case-test-results.xml

  system-integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    needs: [unit-tests, integration-tests]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
      - name: Run system integration tests
        env:
          TAILOPS_TEST_MODE: true
          TAILOPS_INTEGRATION_MODE: true
        run: |
          pytest tests/test_system_integration.py \
            -m integration \
            --junitxml=system-integration-test-results.xml \
            -v
            
      - name: Upload system integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: system-integration-test-results
          path: system-integration-test-results.xml

  load-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install load testing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark locust
          
      - name: Run load tests
        env:
          TAILOPS_LOAD_TEST_MODE: true
        run: |
          pytest tests/test_performance.py \
            -m "load_testing" \
            --junitxml=load-test-results.xml \
            -v --tb=short
            
      - name: Upload load test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: load-test-results
          path: load-test-results.xml

  test-coverage-analysis:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests, edge-case-tests]
    if: always()
    
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v3
        
      - name: Combine coverage reports
        run: |
          pip install coverage
          coverage combine */coverage.xml
          coverage report --show-missing
          coverage html
          
      - name: Upload combined coverage
        uses: actions/upload-artifact@v3
        with:
          name: combined-coverage-report
          path: htmlcov/

  test-quality-gate:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests, edge-case-tests]
    if: always()
    
    steps:
      - name: Download test artifacts
        uses: actions/download-artifact@v3
        
      - name: Quality gate check
        run: |
          # Check test results
          echo "Checking test results..."
          
          # Unit tests must pass
          if [ -f "unit-test-results-*/unit-test-results.xml" ]; then
            echo "Unit tests: PASSED"
          else
            echo "Unit tests: FAILED"
            exit 1
          fi
          
          # Integration tests must pass
          if [ -f "integration-test-results/integration-test-results.xml" ]; then
            echo "Integration tests: PASSED"
          else
            echo "Integration tests: FAILED"
            exit 1
          fi
          
          # Security tests must pass
          if [ -f "security-test-results/security-test-results.xml" ]; then
            echo "Security tests: PASSED"
          else
            echo "Security tests: FAILED"
            exit 1
          fi
          
          echo "All quality gates passed!"
          
      - name: Comment PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            let comment = '## üß™ Test Results Summary\n\n';
            
            // Add unit test results
            if (fs.existsSync('unit-test-results-*/unit-test-results.xml')) {
              comment += '‚úÖ **Unit Tests**: Passed\n';
            } else {
              comment += '‚ùå **Unit Tests**: Failed\n';
            }
            
            // Add integration test results
            if (fs.existsSync('integration-test-results/integration-test-results.xml')) {
              comment += '‚úÖ **Integration Tests**: Passed\n';
            } else {
              comment += '‚ùå **Integration Tests**: Failed\n';
            }
            
            // Add security test results
            if (fs.existsSync('security-test-results/security-test-results.xml')) {
              comment += '‚úÖ **Security Tests**: Passed\n';
            } else {
              comment += '‚ùå **Security Tests**: Failed\n';
            }
            
            comment += '\nüìä Detailed reports are available in the Actions artifacts.';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  nightly-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    timeout-minutes: 300
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
      - name: Run comprehensive test suite
        env:
          TAILOPS_NIGHTLY_MODE: true
        run: |
          pytest tests/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=nightly-test-results.xml \
            -v --tb=short
            
      - name: Upload nightly test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: nightly-test-results-python-${{ matrix.python-version }}
          path: |
            nightly-test-results.xml
            coverage.xml
            htmlcov/

  test-notification:
    runs-on: ubuntu-latest
    needs: [test-quality-gate]
    if: always()
    
    steps:
      - name: Notify test results
        run: |
          if [ "${{ needs.test-quality-gate.result }}" == "success" ]; then
            echo "‚úÖ All tests passed successfully!"
          else
            echo "‚ùå Some tests failed. Please check the test results."
            exit 1
          fi